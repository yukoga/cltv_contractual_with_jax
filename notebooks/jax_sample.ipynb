{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with JAX\n",
    "\n",
    "Ref. [JAX quick start](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)  \n",
    "Ref. [Getting started with JAX (MLPs, CNNs & RNNs)](https://roberttlange.github.io/posts/2020/03/blog-post-10/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.normal(key, (N,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying big matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3000\n",
    "x = random.normal(key, (N, N))\n",
    "x_numpy = np.random.normal(size=(N, N))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100 -r 5 np.dot(x_numpy, x_numpy.T)\n",
    "%timeit -n 100 -r 5 jnp.dot(x, x.T).block_until_ready()\n",
    "%timeit -n 100 -r 5 jnp.dot(x_numpy, x_numpy.T).block_until_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150\n",
    "D_features = 100\n",
    "N_batch = 10\n",
    "\n",
    "x = random.normal(key, (N, D_features))\n",
    "batched_x = random.normal(key, (N_batch, D_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def apply_matrix(x, v):\n",
    "    return jnp.dot(x, v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def naively_batched_apply_matrix(x, v):\n",
    "    return jnp.stack([apply_matrix(x, _v) for _v in v])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(x, batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(x, v):\n",
    "  return jnp.dot(v, x.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(x, batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def v_apply_matrix(v_batched):\n",
    "    return jnp.dot(x, v_batched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched): \n",
    "  return vmap(v_apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.uniform(key, (N_dim, N_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\" Rectified Linear Unit = ReLU activation function. \"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "jit_ReLU = jit(ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time out = ReLU(x).block_until_ready()\n",
    "%time jit_ReLU(x).block_until_ready()\n",
    "%time out2 = jit_ReLU(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_grad(x):\n",
    "    return jnp.array((ReLU(x + 1e-3) - ReLU(x - 1e-3)) / (2 * 1e-3))\n",
    "\n",
    "print(f\"JAX grad: {jit(grad(jit(ReLU)))(2.)}\")\n",
    "print(f\"Finite grad: {finite_grad(2.)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func(x, w):\n",
    "    return w[0] * (x - w[1]) ** 2 + w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 2.\n",
    "b_true = 1.\n",
    "c_true = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-4., 4., 100)\n",
    "y = target_func(x, [a_true, b_true, c_true]) + 2.5*random.normal(key, shape=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(w, x, y):\n",
    "    y_hat = target_func(x, w)\n",
    "    return jnp.sqrt(jnp.sum((y - y_hat)**2)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = jnp.array([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = minimize(rmse_loss, w_init, args=(x, y), method='BFGS',\n",
    "                    tol=1e-7*x.shape[0], options={'maxiter': 20000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"if succeeded: {results.success}\")\n",
    "print(f\"parameters: {results.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "plt.plot(x, target_func(x, results.x))\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmap\n",
    "\n",
    "`vmap` lets you simply write your computations for a single sample case and afterwards wrap it to make it batch compatible.  \n",
    "It is as easy as that. Let’s say you have a 100 dimensional feature vector and want to process it by a linear layer with 512 hidden units & your ReLU activation.  \n",
    "And let’s say you want to compute the layer activations for a batch with size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dim = 10\n",
    "N_hidden_dim = 512\n",
    "N_batch_dim = 32\n",
    "\n",
    "\n",
    "# Generate a batch of vectors to process\n",
    "X = random.normal(key, (N_batch_dim, N_dim))\n",
    "\n",
    "# Generate Gaussian weights and biases\n",
    "params = [random.normal(key, (N_hidden_dim, N_dim)),\n",
    "          random.normal(key, (N_hidden_dim,))]\n",
    "\n",
    "\n",
    "def relu_layer(params, x):\n",
    "    \"\"\" Simple ReLU layer for single sample \"\"\"\n",
    "    return ReLU(jnp.dot(params[0], x) + params[1])\n",
    "\n",
    "\n",
    "def batch_version_relu_layer(params, x):\n",
    "    \"\"\" Error prone batch version \"\"\"\n",
    "    return ReLU(jnp.dot(X, params[0].T) + params[1])\n",
    "\n",
    "\n",
    "def vmap_relu_layer(params, x):\n",
    "    \"\"\" vmap version of ReLU layer \"\"\"\n",
    "    return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = jnp.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_batch_version = batch_version_relu_layer(params, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.all() == out_batch_version.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vmap = vmap_relu_layer(params, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.all() == out_batch_version.all() == out_vmap.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2, 3, 4][:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('3.8.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "673a067c19b0360370b76aae94f77835aaf90ebccf443de00529415a42880203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
